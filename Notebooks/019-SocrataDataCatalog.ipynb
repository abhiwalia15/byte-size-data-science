{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\" ALT=\"BSDS Banner\" WIDTH=1195 HEIGHT=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accessing Socrata Open Data API\n",
    "### discovery\n",
    "\n",
    "The documentation is found at: https://socratadiscovery.docs.apiary.io/#reference\n",
    "\n",
    "Socrates has a discovery API over HTTP requests. We can find data in multiple ways:\n",
    "\n",
    "- by id: each asset has a unique 9 character identifier\n",
    "- by domain: a specific \"repository\" such as \"data.oregon.org\"\n",
    "- by category or tag: assets may be labeled with zero or more categories and zero or more tags\n",
    "- by type: api, calendar, chart, datalens, dataset, federated_href, file, filter, form, href, link, map, measure, story, visualization\n",
    "- by domain-specific metadata\n",
    "- by attribution\n",
    "- by license\n",
    "- by query term: any text found in name, description, category, tags, column names, column fieldnames, column descriptions, attribution\n",
    "- by parent ID\n",
    "- by provenance\n",
    "- by owner\n",
    "- by granted shares\n",
    "- by column name\n",
    "- by visibility\n",
    "- by public/private asset\n",
    "- . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 019-finding Data: Socrata Catalog\n",
    "Execute the next cell if you want to see the `Byte Size Data Science` youtube channel video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.youtube.com/embed/D46A9r3bfjM?rel=0&amp;controls=0&amp;showinfo=0\", width=560, height=315)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed in the notebook\n",
    "import urllib3, requests, json\n",
    "import pandas as pd\n",
    "\n",
    "# pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domains\n",
    "There are two URLs used to discover domains:\n",
    "- for Americas: http://api.us.socrata.com/api/catalog/v1 (Canada, Columbia, Mexico, USA, . . .)\n",
    "- for Europe: http://api.eu.socrata.com/api/catalog/v1 (Italy, Netherland, Spain, UK, Others)\n",
    "\n",
    "Let's get a list of domains that has assets and the count of assets in each domain.\n",
    "\n",
    "**Note that not all assets are datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding domains in Americas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_us=\"http://api.us.socrata.com/api/catalog/v1\"\n",
    "urlDomains = url_us + \"/domains\"\n",
    "response = requests.get(urlDomains)\n",
    "jsondoc = json.loads(response.text)\n",
    "domains_us_df = pd.io.json.json_normalize(jsondoc['results'])\n",
    "# Number of entries/number of domains\n",
    "print('Number of domains: ' + str(domains_us_df.shape[0]))\n",
    "print('Number of assets: ' + str(domains_us_df['count'].agg('sum')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 domains\n",
    "domains_us_df.sort_values('count', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding domains from the European URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_eu = \"http://api.eu.socrata.com/api/catalog/v1\" \n",
    "urlDomains = url_eu + \"/domains\"\n",
    "response = requests.get(urlDomains)\n",
    "jsondoc = json.loads(response.text)\n",
    "domains_eu_df = pd.io.json.json_normalize(jsondoc['results'])\n",
    "# Number of entries/number of domains\n",
    "print('Number of domains: ' + str(domains_eu_df.shape[0]))\n",
    "print('Number of assets: ' + str(domains_eu_df['count'].agg('sum')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 26 domains (right now), list them all\n",
    "domains_eu_df.sort_values('count', ascending=False).head(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching by type - datasets\n",
    "We can select assets by types using the `only` parameter: api, calendar, chart, datalens, dataset, federated_href, file, filter, form, href, link, map, measure, story, visualization\n",
    "\n",
    "We're interested in datasets.\n",
    "\n",
    "We can also search by categories Here are some available categories:<br/>\n",
    "demographics, economy, education, environment, finance, health, housing & development, infrastructure, politics, public safety, recreation, transportation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, it returns 100 records.\n",
    "# We can get more by using pagination parameters: offset and limit (up to 10000 records)\n",
    "# If there are more, we have to use the scroll_id parameter\n",
    "# the ID of the last result in the previously fetched chunk of results. \n",
    "\n",
    "url=\"http://api.us.socrata.com/api/catalog/v1\"\n",
    "# Retrieve only datasets\n",
    "urldatasets = url + \"?only=dataset\"\n",
    "offset=0\n",
    "limit=10000\n",
    "limit2=100\n",
    "done = 0\n",
    "scroll_id=\"\"\n",
    "all_records = dict(results=[])\n",
    "while (done == 0) :\n",
    "    page = \"&scroll_id=\" + scroll_id + \"&limit=\" + str(limit)\n",
    "    # print(url + page)\n",
    "    response = requests.get(urldatasets + page)\n",
    "    if response.status_code != 200 :\n",
    "        print(response.status_code)\n",
    "        done = 1\n",
    "        break\n",
    "    if (offset == 0) :\n",
    "        jsondoc = json.loads(response.text)\n",
    "        scroll_id = jsondoc['results'][len(jsondoc['results']) - 1]['resource']['id']\n",
    "        max_records = jsondoc['resultSetSize']\n",
    "        for val in jsondoc['results'] :\n",
    "            all_records['results'].append(val)\n",
    "    else :\n",
    "        jsondoc = json.loads(response.text)\n",
    "        scroll_id = jsondoc['results'][len(jsondoc['results']) - 1]['resource']['id']\n",
    "        for val in jsondoc['results'] :\n",
    "            all_records['results'].append(val)\n",
    "\n",
    "    offset += limit\n",
    "    if (offset >= max_records) :\n",
    "        done = 1\n",
    "\n",
    "catalog_df = pd.io.json.json_normalize(all_records['results'])\n",
    "catalog_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List domains with the most datasets\n",
    "You can find the meaning of the domain suffix at: https://en.wikipedia.org/wiki/List_of_Internet_top-level_domains\n",
    "\n",
    "Some suffixes:\n",
    "- .au : Australia\n",
    "- .ca : Canada\n",
    "- .co : Columbia\n",
    "- .com: Commercial entities\n",
    "- .edu: Education\n",
    "- .gov: US national and state\n",
    "- .org: Possibly non-profit but open (some cities use it)\n",
    "- .us : US cities?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the top domains and the number of datasets available\n",
    "# you can find the domain name postfix meaning at:\n",
    "# https://en.wikipedia.org/wiki/List_of_Internet_top-level_domains\n",
    "\n",
    "print(\"Number of domains: \" + str(catalog_df['metadata.domain'].nunique()))\n",
    "\n",
    "catalog_df.groupby(['metadata.domain']).agg(['count'])['link'].\\\n",
    "           sort_values('count',ascending=False).reset_index(drop=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the bottom 30 domains?\n",
    "catalog_df.groupby(['metadata.domain']).agg(['count'])['link'].\\\n",
    "           sort_values('count',ascending=True).reset_index(drop=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count for the European URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, it returns 100 records.\n",
    "# We can get more by using pagination parameters: offset and limit (up to 10000 records)\n",
    "# If there are more, we have to use the scroll_id parameter\n",
    "# the ID of the last result in the previously fetched chunk of results. \n",
    "\n",
    "url_eu = \"http://api.eu.socrata.com/api/catalog/v1\"\n",
    "# Retrieve only datasets\n",
    "urldatasets = url_eu + \"?only=dataset\"\n",
    "offset=0\n",
    "limit=10000\n",
    "limit2=100\n",
    "done = 0\n",
    "scroll_id=\"\"\n",
    "all_records = dict(results=[])\n",
    "while (done == 0) :\n",
    "    page = \"&scroll_id=\" + scroll_id + \"&limit=\" + str(limit)\n",
    "    # print(url + page)\n",
    "    response = requests.get(urldatasets + page)\n",
    "    if response.status_code != 200 :\n",
    "        print(response.status_code)\n",
    "        done = 1\n",
    "        break\n",
    "    if (offset == 0) :\n",
    "        jsondoc = json.loads(response.text)\n",
    "        scroll_id = jsondoc['results'][len(jsondoc['results']) - 1]['resource']['id']\n",
    "        max_records = jsondoc['resultSetSize']\n",
    "        for val in jsondoc['results'] :\n",
    "            all_records['results'].append(val)\n",
    "    else :\n",
    "        jsondoc = json.loads(response.text)\n",
    "        scroll_id = jsondoc['results'][len(jsondoc['results']) - 1]['resource']['id']\n",
    "        for val in jsondoc['results'] :\n",
    "            all_records['results'].append(val)\n",
    "\n",
    "    offset += limit\n",
    "    if (offset >= max_records) :\n",
    "        done = 1\n",
    "\n",
    "catalog_eu_df = pd.io.json.json_normalize(all_records['results'])\n",
    "catalog_eu_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Americas\n",
    "We need to explore the different categories. The first thig to note is that many records don't have entries in each attribute.\n",
    "\n",
    "`classification.categories`, `classification.domain_category`, `classification.domain_metadata`, `classification.domain_tags`, `classification.tags`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records include classification.categories?\n",
    "# We find out by looking at which record does not have an empty list\n",
    "print('classification.categories: ' + \n",
    "      str(catalog_df[catalog_df['classification.categories'].map(len) > 0]['classification.categories'].count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that several entries don't have categories.\n",
    "# List the categories found in the datasets listed\n",
    "my_dict = dict()\n",
    "for classification in catalog_df['classification.categories'] :\n",
    "    for val in classification :\n",
    "        if val in my_dict :\n",
    "            my_dict[val] += 1\n",
    "        else :\n",
    "            my_dict[val] = 1\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification.domain_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This columns seems to be string or float when empty\n",
    "import operator\n",
    "from heapq import nlargest\n",
    "from operator import itemgetter\n",
    "\n",
    "my_dict2 = dict()\n",
    "for val in catalog_df['classification.domain_category'] :\n",
    "    if (isinstance(val, float) == False) :\n",
    "        if val in my_dict2 :\n",
    "            my_dict2[val] += 1\n",
    "        else :\n",
    "            my_dict2[val] = 1\n",
    "print(\"Number of entries: \" + str(len(my_dict2)))\n",
    "print(\"\")\n",
    "# sorted_my_dict2 = sorted(my_dict.items(), key=operator.itemgetter(1))\n",
    "for name, score in nlargest(20, my_dict2.items(), key=itemgetter(1)):\n",
    "    print (name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification.domain_metadata\n",
    "The meatadata section is a list of dictionaries. For example:\n",
    "\n",
    "`{'key': 'Update_Update-Frequency', 'value': 'Monthly'}`<br/>\n",
    "`{'key': 'Dataset-Summary_Granularity', 'value': ''}`\n",
    "\n",
    "It is worth explring the metadata available to search on the characteristics of the daatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entries with metadata\n",
    "print('classification.domain_metadata: ' + \n",
    "      str(catalog_df[catalog_df['classification.domain_metadata'].map(len) > 0]['classification.domain_metadata'].count() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dictionaries\n",
    "my_dict3 = dict()\n",
    "for metadata in catalog_df['classification.domain_metadata'] :\n",
    "    for val in metadata :\n",
    "        if val['key'] in my_dict3 :\n",
    "            my_dict3[val['key']] += 1\n",
    "        else :\n",
    "            my_dict3[val['key']] = 1\n",
    "print(\"Number of entries: \" + str(len(my_dict2)))\n",
    "print(\"\")\n",
    "# sorted_my_dict3 = sorted(my_dict3.items(), key=operator.itemgetter(1))\n",
    "for name, score in nlargest(20, my_dict3.items(), key=itemgetter(1)):\n",
    "    print (name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification.domain_tags\n",
    "Tags can be used to limit the number of datasets we want to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of domain_tags\n",
    "print('classification.domain_tags: ' +\n",
    "      str(catalog_df[catalog_df['classification.domain_tags'].map(len) > 0]['classification.domain_tags'].count() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict4 = dict()\n",
    "for tags in catalog_df['classification.domain_tags'] :\n",
    "    for val in tags :\n",
    "        if val in my_dict4 :\n",
    "            my_dict4[val] += 1\n",
    "        else :\n",
    "            my_dict4[val] = 1\n",
    "print(\"Number of entries: \" + str(len(my_dict4)))\n",
    "print(\"\")\n",
    "# sorted_my_dict4 = sorted(my_dict.items(), key=operator.itemgetter(1))\n",
    "for name, score in nlargest(20, my_dict4.items(), key=itemgetter(1)):\n",
    "    print (name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification.tags\n",
    "This field appears to be mostly empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('classification.tags: ' +\n",
    "      str(catalog_df[catalog_df['classification.tags'].map(len) > 0]['classification.tags'].count() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict5 = dict()\n",
    "for tags in catalog_df['classification.tags'] :\n",
    "    for val in tags :\n",
    "        if val in my_dict5 :\n",
    "            my_dict5[val] += 1\n",
    "        else :\n",
    "            my_dict5[val] = 1\n",
    "print(\"Number of entries: \" + str(len(my_dict5)))\n",
    "print(\"\")\n",
    "# sorted_my_dict5 = sorted(my_dict.items(), key=operator.itemgetter(1))\n",
    "for name, score in nlargest(20, my_dict5.items(), key=itemgetter(1)):\n",
    "    print (name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the records that use tags\n",
    "catalog_df[catalog_df['classification.tags'].map(len) > 0][\n",
    "    ['classification.tags','resource.name','resource.description','metadata.domain','resource.id']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of non-null values per attributes\n",
    "We start with a simple count. This is fine for some attributes but some attributes are list or dictionaries. This requires looking also at sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other fields\n",
    "There are multiple other fields that could be used to limit the search. For example:\n",
    "- metadata.domain\n",
    "- resource.columns_name\n",
    "- resource.createdAt\n",
    "- resource.name\n",
    "- resource.description\n",
    "- resource.updatedAt\n",
    "\n",
    "Of those, the domain name and updatedAt attributes should be the most useful.\n",
    "\n",
    "Other fields such as page view statistics could potentially be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp\n",
    "An important search criterion is a date such as date updated.<br/>\n",
    "This way we can ask for datasets updated at least in the last 3 months for example.\n",
    "\n",
    "For that, we have to make sure the date fields have the proper data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date field to timestamps\n",
    "catalog_df['resource.createdAt'] = \\\n",
    "           catalog_df['resource.createdAt'].apply(pd.to_datetime, infer_datetime_format=True, errors='coerce')\n",
    "catalog_df['resource.updatedAt'] = \\\n",
    "           catalog_df['resource.updatedAt'].apply(pd.to_datetime, infer_datetime_format=True, errors='coerce')\n",
    "catalog_df[['resource.name','metadata.domain','resource.id','resource.createdAt',\n",
    "            'resource.updatedAt','resource.description','link','resource.columns_field_name', \n",
    "            'resource.columns_datatype']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df[['resource.createdAt','resource.updatedAt']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for chicago datasets that talk about crashes\n",
    "catalog_df[catalog_df['metadata.domain'] == 'data.cityofchicago.org'][\n",
    "    catalog_df['resource.description'].str.contains(\"[Cc]rash\") == True][\n",
    "    ['resource.name','resource.id','resource.description']].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset\n",
    "We can keep retrieving the dataset from Socrata. It makes sense since this way we get all the updates.<br/>\n",
    "There is a limit on the number of queries and volume of data that can be retrieve so avoiding unnecessary queries makes sense.\n",
    "\n",
    "We can work with a fix list and work from that for a while. We can always re-create the list regularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't write out the Pandas DataFrame index\n",
    "catalog_df.to_csv(\"./SocrataCatalog.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to COS\n",
    "Write the local file to the Cloud Object Storage\n",
    "\n",
    "**Your credentials will be different from the ones listed here**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials = {\n",
    "    'IBM_API_KEY_ID': '9OUS8qWWwiqFtRkeH6njgVaar',\n",
    "    'IAM_SERVICE_ID': 'iam-ServiceI4f37-abd9-38b10084d177',\n",
    "    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.bluemix.net/oidc/token',\n",
    "    'BUCKET': 'pr-paqxy5fmsmaykn'\n",
    "}\n",
    "cos = ibm_boto3.client(service_name='s3',\n",
    "                       ibm_api_key_id=credentials[\"IBM_API_KEY_ID\"],\n",
    "                       ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "                       config=Config(signature_version='oauth'),\n",
    "        endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "Bucket=credentials[\"BUCKET\"]\n",
    "\n",
    "cos.upload_file('./SocrataCatalog.csv', Bucket, 'SocrataCatalog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

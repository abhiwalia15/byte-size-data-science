{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Decision Tree model for Churn Analysis\n",
    "Spark comes with multiple popular machine learning algorithms. In this tutorial, we are looking at decision trees to determine the churn of telco customers. Decision trees are easier to understand in concept than many other machine learning algorithms since many people have been exposed to decisions such as: if this, then that, else another thing. We can easily understand how decisions are made this way through divide and conquer.\n",
    "\n",
    "Decision trees are more than evaluating an attribute value and decide what to do next. This algorithm looks at the input data and decides how significant each attribute is, how it defines grouping between multiple records. Once this analysis is done it can decide which attribute nd which value range can lead to a decision. \n",
    "\n",
    "We start by getting a Spark session and reading the data into a DataFrame. The `data_df.show(3)` forces the instantiation of the data and provides a formatted view of it. Other methods could have been used, as seen in lab 1, such as the `take` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://github.com/jacquesroy/byte-size-data-science/raw/master/data/customer_churn.csv'\n",
    "\n",
    "filename = url.rsplit('/', 1)[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Add asset from file system\n",
    "data_df = spark.read.csv(filename, header='true', inferSchema = 'true')\n",
    "data_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the schema. See that data types were inferred\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data before creating the model\n",
    "Some columns have discrete string values: Gender, Status, Car Owner, and so on. <br/>\n",
    "We use a __`StringIndexer`__ to convert the values to numbers.\n",
    "\n",
    "We also convert the 17 columns into a vector so all \"features\" are in one column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the indexers\n",
    "Converting the discrete values to index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline, Model\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "churn_indexer = StringIndexer(inputCol=\"CHURN\", outputCol=\"label\").fit(data_df)\n",
    "gender_indexer = StringIndexer(inputCol=\"Gender\", outputCol=\"IXGender\")\n",
    "status_indexer = StringIndexer(inputCol=\"Status\", outputCol=\"IXStatus\")\n",
    "car_indexer = StringIndexer(inputCol=\"Car Owner\", outputCol=\"IXCarOwner\")\n",
    "pay_indexer = StringIndexer(inputCol=\"Paymethod\", outputCol=\"IXPaymethod\")\n",
    "localbill_indexer = StringIndexer(inputCol=\"LocalBilltype\", outputCol=\"IXLocalBilltype\")\n",
    "long_indexer = StringIndexer(inputCol=\"LongDistanceBilltype\", outputCol=\"IXLongDistanceBilltype\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the conversion of columns to vector\n",
    "Note the following statement:<br/>\n",
    "`dt = DecisionTreeClassifier(maxDepth=4, labelCol=\"label\")`\n",
    "\n",
    "In this statement we limit the depth of the tree to 4. This is an arbitrary value that could be changed. It limits the granularity of the decision and can help avlid what is called **overfitting**. This is an important concept that you may want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(inputCols=[\"ID\", \"IXGender\", \"IXStatus\", \"Children\", \"Est Income\", \"IXCarOwner\", \"Age\", \n",
    "               \"LongDistance\", \"International\", \"Local\", \"Dropped\", \"IXPaymethod\", \"IXLocalBilltype\", \n",
    "               \"IXLongDistanceBilltype\", \"Usage\", \"RatePlan\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "dt = DecisionTreeClassifier(maxDepth=4, labelCol=\"label\")\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=churn_indexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline that converts the data\n",
    "A pipeline is the set of steps that were defined earlier that are put together in a series of processing steps. We then apply the pipeline to data to create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[churn_indexer, gender_indexer, status_indexer, car_indexer, pay_indexer, \n",
    "                               localbill_indexer, long_indexer, vectorAssembler_features, dt, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "Note that we split the input data into training data to create the model and testing data to evaluate its accuracy. In many cases, it is split into three groups with a validation group that can be used to see if the mode is degrading over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select records and get to 80% of the data in training_df and 20% in testing_df\n",
    "(training_df, testing_df) = data_df.randomSplit([0.80, 0.20], 123)\n",
    "model = pipeline_rf.fit(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model accuracy\n",
    "The model fits the training data. We can tests the accuracy of the model on data that was not part of the model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(testing_df)\n",
    "evaluatorRF = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "accuracy = evaluatorRF.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model\n",
    "We can save the model to the local filesystem. The model is saved as a directory structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('DecisionTreeChurnModel')\n",
    "%ls -R DecisionTreeChurnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "model2 = PipelineModel.load('DecisionTreeChurnModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it to see that we get the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.transform(testing_df)\n",
    "evaluatorRF = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "accuracy = evaluatorRF.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Repository Services to Save and Load Model\n",
    "A model can be created in one Notebook or through the Watson Studio \"Model\" creation, and reused in another notebook. It is even possible to publish it and use it through a REST API.\n",
    "\n",
    "A model can be created in one Notebook or through the Watson Studio \"Model\" creation, and re-used in another notebook.\n",
    "\n",
    "For this, we need to save the model and create a deployment\n",
    "\n",
    "See the documentation at: `http://watson-ml-api.mybluemix.net/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install watson-machine-learning-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "  \"apikey\": \"sEF2bGvVooTpOhJSjyLJ2VBM01itYiD8tDpJG_4Ba5l5\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:pm-20:us-south:a/e46675b7f1bf89b09b5badfb3bd4a7b5:0c4bc8b4-ec84-4a68-b9ca-59597b23af4b::\",\n",
    "  \"iam_apikey_name\": \"auto-generated-apikey-be7374c9-f851-4c1d-ba62-5b8ff10fc11b\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/e46675b7f1bf89b09b5badfb3bd4a7b5::serviceid:ServiceId-c0bc47e6-6a85-46f7-905a-efe757ee1ae1\",\n",
    "  \"instance_id\": \"0c4bc8b4-ec84-4a68-b9ca-59597b23af4b\",\n",
    "  \"password\": \"b6e74b34-1ce7-440a-9a48-5c7c6d6865b7\",\n",
    "  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "  \"username\": \"be7374c9-f851-4c1d-ba62-5b8ff10fc11b\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "print(client.service_instance.get_url())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List models already in the repository\n",
    "client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = client.repository.store_model(model=model, meta_props={'name':'Telco Churn Prediction Model'}, \n",
    "                                            training_data=training_df, pipeline=pipeline_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List models in the repository\n",
    "client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model UID\n",
    "model_uid = client.repository.get_model_uid(saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_details = client.deployments.create(model_uid, \"Deployment of Telco churn model\", deployment_type='online')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing a Saved Model\n",
    "We can get a list of all the models available using. It is then possible to iterate on each one and find the name we are looking for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allModels = client.deployments.get_details()['resources']\n",
    "modelUid = \"\"\n",
    "deploymentDetail = \"\"\n",
    "for model in allModels :\n",
    "    print (\"Model name: \" + model['entity']['deployable_asset']['name'])\n",
    "    print (\"Model uid : \" + model['entity']['deployable_asset']['guid'])\n",
    "    if (model['entity']['deployable_asset']['name'] == \"Telco Churn Prediction Model\"):\n",
    "        modelUid = model['entity']['deployable_asset']['guid']\n",
    "        deploymentDetail = model\n",
    "        print(\"Deployed model: \" + model['entity']['name'])\n",
    "\n",
    "print (\"\\nmodelUid: \" + modelUid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the model artifact\n",
    "If we already have the model uid, we can get it using the get command as shown below.<br/>\n",
    "Since we did not get the `ModelArtifact` in the previous cell, we still need to execute the following one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_url = client.deployments.get_scoring_url(deploymentDetail)\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the attributes used in the model\n",
    "modelDetail = client.repository.get_model_details(model_uid)\n",
    "# print(md)\n",
    "vals=[]\n",
    "for attr in modelDetail['entity']['input_data_schema']['fields'] :\n",
    "    vals.append(attr['name'])\n",
    "print(*vals, sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the model\n",
    "scoring_payload = {'fields': ['ID','Gender','Status','Children','Est Income','Car Owner',\n",
    "                              'Age','LongDistance','International','Local','Dropped',\n",
    "                              'Paymethod','LocalBilltype','LongDistanceBilltype',\n",
    "                              'Usage','RatePlan'], \n",
    "                   'values': [[1,'F','S',1.0,38000.0,'N',24.393333,23.56,0.0,206.08,0.0,'CC','Budget','Intnl_discount',229.64,3.0],                      \n",
    "                              [6,'M','M',2.0,29616.0,'N',49.426667,29.78,0.0,45.5,0.0,  'CH','FreeLocal','Standard',75.29,2.0]\n",
    "                             ]}\n",
    "predictions = client.deployments.score(scoring_url, scoring_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction in predictions['values'] :\n",
    "    print(\"ID: \" + str(prediction[0]) + \", probability: [\" + \n",
    "          str(prediction[26][0]) + ',' +  str(prediction[26][1]) + \n",
    "          \"], prediction: \" + str(prediction[27]) + \", predicted label: \" + str(prediction[28])\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing a saved model\n",
    "We can remove a model from the repository using the __`remove`__ method.<br/>\n",
    "In the example below, we show that since we only had one model in the repository, the looping over the models does not show anything once the model has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the deployment\n",
    "deployment_uid = client.deployments.get_uid(deploymentDetail)\n",
    "client.deployments.delete(deployment_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Telco Churn Prediction Model' model(s)\n",
    "for mldef in client.repository.get_details()['models']['resources'] :\n",
    "    if (mldef['entity']['name'] == 'Telco Churn Prediction Model') :\n",
    "        ml_uid = client.repository.get_model_uid(mldef)\n",
    "        client.repository.delete(ml_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Telco Churn Prediction Model' definition(s)\n",
    "for mldef in client.repository.get_details()['definitions']['resources'] :\n",
    "    if (mldef['entity']['name'] == 'Telco Churn Prediction Model') :\n",
    "        ml_uid = client.repository.get_definition_uid(mldef)\n",
    "        client.repository.delete(ml_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

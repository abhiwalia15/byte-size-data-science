{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming - Lemminizer\n",
    "About stemming, see: https://en.wikipedia.org/wiki/Stemming\n",
    "\n",
    "See also: \n",
    "https://www.nltk.org/<br/>\n",
    "https://pypi.python.org/pypi/stemming/1.0![image.png](attachment:image.png)<br/>\n",
    "https://pythonspot.com/category/nltk/<br/>\n",
    "https://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') # needed for the word_tokenize method\n",
    "nltk.download('averaged_perceptron_tagger') # needed for part of speech\n",
    "nltk.download('wordnet') # Needed for lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"game Gaming gamed games\"\n",
    "words = word_tokenize(data)\n",
    "\n",
    "ps = PorterStemmer()\n",
    " \n",
    "for word in words:\n",
    "    print(word + \" --> \" + ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Stemmers in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=\"game Gaming gamed games is are was were man men wolves abaci good better best surprise surprisingly\"\n",
    "words = word_tokenize(data)\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "sl = SnowballStemmer('english') # or nltk.stem.snowball.EnglishStemmer(ignore_stopwords=False)\n",
    "table=[]\n",
    "for word in words:\n",
    "    table.append([word , ps.stem(word),ls.stem(word),sl.stem(word)])\n",
    "    \n",
    "table_pd = pd.DataFrame(table, columns=[\"Word\", \"Porter\", \"Lancaster\", \"Snowball\"])\n",
    "table_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech tags:\n",
    "<table align=\"left\" style=\"width:80%\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">CC\tcoordinating conjunction</td>\n",
    "        <td style=\"text-align: left;\">CD\tcardinal digit</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">DT\tdeterminer</td>\n",
    "        <td style=\"text-align: left;\">EX\texistential there (like: \"there is\" / \"there exists\")</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">FW\tforeign word</td>\n",
    "        <td style=\"text-align: left;\">IN\tpreposition/subordinating conjunction</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">JJ\tadjective\t'big'</td>\n",
    "<td style=\"text-align: left;\">JJR\tadjective, comparative\t'bigger'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">JJS\tadjective, superlative\t'biggest'</td>\n",
    "        <td style=\"text-align: left;\">LS\tlist marker\t1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">MD\tmodal\tcould, will</td>\n",
    "        <td style=\"text-align: left;\">NN\tnoun, singular 'desk'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">NNS\tnoun plural\t'desks'</td>\n",
    "        <td style=\"text-align: left;\">NNP\tproper noun, singular\t'Harrison'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">NNPS\tproper noun, plural\t'Americans'</td>\n",
    "        <td style=\"text-align: left;\">PDT\tpredeterminer\t'all the kids'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">POS\tpossessive ending\tparent's</td>\n",
    "        <td style=\"text-align: left;\">PRP\tpersonal pronoun\tI, he, she</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td style=\"text-align: left;\">PRP\\$\tpossessive pronoun\tmy, his, hers</td>\n",
    "<td style=\"text-align: left;\">RB\tadverb\tvery, silently,</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">RBR\tadverb, comparative\tbetter</td>\n",
    "<td style=\"text-align: left;\">RBS\tadverb, superlative\tbest</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">RP\tparticle\tgive up</td>\n",
    "<td style=\"text-align: left;\">TO\tto\tgo 'to' the store.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">UH\tinterjection\terrrrrrrrm</td>\n",
    "<td style=\"text-align: left;\">VB\tverb, base form\ttake</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">VBD\tverb, past tense\ttook</td>\n",
    "<td style=\"text-align: left;\">VBG\tverb, gerund/present participle\ttaking</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">VBN\tverb, past participle\ttaken</td>\n",
    "<td style=\"text-align: left;\">VBP\tverb, sing. present, non-3d\ttake</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">VBZ\tverb, 3rd person sing. present\ttakes</td>\n",
    "<td style=\"text-align: left;\">WDT\twh-determiner\twhich</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">WP\twh-pronoun\twho, what</td>\n",
    "<td style=\"text-align: left;\">WP\\$\tpossessive wh-pronoun\twhose</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left;\">WRB\twh-abverb\twhere, when</td>\n",
    "        <td style=\"text-align: left;\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"game gaming gamed games is are was were man men wolves abaci good better best surprise surprisingly\"\n",
    "words = word_tokenize(data)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(word_tokenize(\"the wolves of wall street did not bring their abaci\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech for the lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between pos_tag and lemmatizer\n",
    "# When in doubt, just put noun\n",
    "posMapping = { \"CC\": wn.NOUN, \"CD\": wn.NOUN, \"DT\": wn.NOUN, \"EX\": wn.NOUN,\n",
    "               \"FW\": wn.NOUN, \"IN\": wn.NOUN, \"JJ\": wn.ADJ,  \"JJR\": wn.ADJ,\n",
    "               \"JJS\": wn.ADJ, \"LS\": wn.NOUN, \"MD\": wn.VERB, \"NN\": wn.NOUN,\n",
    "               \"NNS\": wn.NOUN, \"NNP\": wn.NOUN, \"NNPS\": wn.NOUN, \"PDT\": wn.NOUN,\n",
    "               \"POS\": wn.NOUN, \"PRP\": wn.NOUN, \"PRP$\": wn.NOUN, \"RB\": wn.ADV,\n",
    "               \"RBR\": wn.ADV, \"RBS\": wn.ADV, \"RP\": wn.NOUN, \"TO\": wn.NOUN,\n",
    "               \"UH\": wn.NOUN, \"VB\": wn.VERB,\"VBD\": wn.VERB, \"VBG\": wn.VERB,\n",
    "               \"VBN\": wn.VERB, \"VBP\": wn.VERB, \"VBZ\": wn.VERB, \"WDT\": wn.NOUN,\n",
    "               \"WP\": wn.NOUN, \"WP$\": wn.NOUN, \"WRB\": wn.ADV   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "words = word_tokenize(data)\n",
    "result = nltk.pos_tag(words)\n",
    "\n",
    "for word in result:\n",
    "    print(\"  \" + word[0] + \"(\" +  word[1] + \") --> \" + lemmatiser.lemmatize(word[0], pos=posMapping[word[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nltk.pos_tag(word_tokenize(\"the wolves of wall street did not bring their abaci\"))\n",
    "\n",
    "for word in result:\n",
    "    print(\"  \" + word[0] + \"(\" +  word[1] + \") --> \" + lemmatiser.lemmatize(word[0], pos=posMapping[word[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

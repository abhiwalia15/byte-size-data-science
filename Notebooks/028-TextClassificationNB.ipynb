{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\" ALT=\"BSDS Banner\" WIDTH=1195 HEIGHT=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes document classification\n",
    "[Notebook companion to: http://youtube.com/c/ByteSizeDataScience]\n",
    "\n",
    "In this notebook, we use an embedded encoding to see if we can group documents.\n",
    "\n",
    "The documents are a subset of a dataset found at: `http://disi.unitn.it/moschitti/corpora.htm`\n",
    "\n",
    "It refers to Reuters-21578 90 categories set. I took a subset of the data that is available at `http://disi.unitn.it/moschitti/corpora/Reuters21578-Apte-90Cat.tar.gz`\n",
    "\n",
    "I am using a total of 10 classifications.files that contained the word blah (as in Blah, blah, blah.).\n",
    "```\n",
    "cd Reuters21578-Apte-90Cat/training\n",
    "for i in `ls -d *`\n",
    "do\n",
    "    for j in `grep -l -i  blah $i/*`\n",
    "    do\n",
    "        rm $j\n",
    "    done\n",
    "done\n",
    "```\n",
    "\n",
    "Before creating the dataset, I removed the \n",
    "\n",
    "To create the dataset, I used the following script (modify it for your directory structure):\n",
    "```\n",
    "mkdir Reuters2\n",
    "cd Reuters21578-Apte-90Cat/training\n",
    "for i in \"alum\" \"barley\" \"coffee\" \"gold\" \"housing\" \"lead\" \"retail\" \"rubber\" \"tin\" \"wheat\"\n",
    "do\n",
    "   cd $i\n",
    "   for f in `ls *`\n",
    "   do\n",
    "     cp $f ../../../Reuters2/$i.$f.txt\n",
    "   done\n",
    "   cd ..\n",
    "done\n",
    "```\n",
    "\n",
    "Then I zipped the content of the Reuters2 directory to create Reuters.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 028-Document Classification using Naive Bayes\n",
    "Execute the next cell if you want to see the `Byte Size Data Science` youtube channel video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.youtube.com/embed/6SpLv8zlnPk?rel=0&amp;controls=0&amp;showinfo=0\", width=560, height=315)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "The original dataset includes 572 documents.\n",
    "\n",
    "The classification is part of the filename. The format is:<br/>\n",
    "`<fileclass>.<filename>.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files from a zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "client = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='9OBEPHS0jp5qEFWpF-US8qWWwiqFtRkeH6njgVaar',\n",
    "    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about your possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.txt Reuters.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.download_file(Bucket='bscstesting-donotdelete-pr-paqxy5fmsmaykn', \n",
    "                     Key='Reuters.zip', Filename='Reuters.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip Reuters.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf __MACOSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayesSet0 = sc.wholeTextFiles(\"./*.txt\")\n",
    "NaiveBayesSet0.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Convert the input data</b><br/>\n",
    "We need to split the path into a fileclass and filename and convert the input<br/>\n",
    "into a three-element array containing:<br/>\n",
    "&lt;fileclass>, &lt;filename>, &lt;content>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayesSet = NaiveBayesSet0.map(\n",
    "   lambda tuple: ((tuple[0].split(\"/\")[-1]).split(\".\", 1)[0], \n",
    "                  (tuple[0].split(\"/\")[-1]).split(\".\", 1)[1],tuple[1])\n",
    "    )\n",
    "# NaiveBayesSet.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a DataFrame so we can use SQL to manipulate the data\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "classNameContent = StructType([StructField(\"fileclass\", StringType(), True),\n",
    "                               StructField(\"filename\",  StringType(), True),\n",
    "                               StructField(\"content\",   StringType(), True)])\n",
    "FinalDataSet = spark.createDataFrame(NaiveBayesSet, classNameContent)\n",
    "FinalDataSet.registerTempTable(\"articles\")\n",
    "\n",
    "print(\"Total number of articles: \" + str(FinalDataSet.count()) )\n",
    "spark.sql(\n",
    "    \"select fileclass, count(filename) as cnt \" +\n",
    "    \"from articles \" +\n",
    "    \"group by fileclass \"\n",
    "    \"order by fileclass limit 20\" ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation: Tokenizing\n",
    "We want to split the articles by non-word characters.\n",
    "\n",
    "We could use the Tokenizer class:\n",
    "\n",
    "```from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "result = tokenizer.transform(FinalDataSet)\n",
    "```\n",
    "\n",
    "But this keeps the punctuation so we'll do it differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split all the text files using non-Word characters and see what we get.\n",
    "import re\n",
    "AllTokensNonWordSplit = FinalDataSet.select('content').rdd.flatMap(\n",
    "                        lambda text: re.findall(r\"[\\w']+\", text.content.lower()) )\n",
    "\n",
    "# We see over 250 thousand tokens\n",
    "print(\"Number of tokens: \" + str(AllTokensNonWordSplit.count()) )\n",
    "print(\"Number of distinct tokens: \" + str(AllTokensNonWordSplit.distinct().count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 30 words\n",
    "for x in AllTokensNonWordSplit.distinct().take(30) : print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "Let's look at the word distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some SQL to find the most common token.\n",
    "tokens = StructType([StructField(\"token\",  StringType(), True)])\n",
    "\n",
    "# Create a dataframe from the AllTokensNonWordSplit RDD\n",
    "AllTokens = spark.createDataFrame(\n",
    "                   AllTokensNonWordSplit.map(lambda x:[x]), tokens )\n",
    "\n",
    "AllTokens.registerTempTable(\"Tokens\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select token, count(token) tokencount \n",
    "    from Tokens \n",
    "    group by token \n",
    "    order by tokencount desc \n",
    "    limit 20\n",
    "    \"\"\").toPandas().head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n",
    "We see that some words are very common. Many of them are likely in every document.\n",
    "They don't add anything in terms of classification then.\n",
    "\n",
    "These type of words are called stop words. If we remove them from the documents, we would end up\n",
    "with smaller documents leading to faster execution. We don't need to do this. \n",
    "We'll use a different method to assign weights to the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation\n",
    "Creating the labeled data points RDDs and running the model.\n",
    "\n",
    "The first step is to convert the labels, document classes, to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class\n",
    "classes=[\"alum\", \"barley\", \"coffee\", \"gold\", \"housing\", \"lead\", \"retail\", \"rubber\", \"tin\", \"wheat\"]\n",
    "classIx=[0,1,2,3,4,5,6,7,8,9]\n",
    "classLookupMap=dict(zip(classes,classIx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the ceontent and convert the fileclass to a number\n",
    "import re\n",
    "from pyspark.sql import Row\n",
    "# Convert content to array of words.\n",
    "AllTokens_df = FinalDataSet.rdd.map(lambda text: Row(fileclass=classLookupMap[text[0]], filename=text[1], \n",
    "                                                     content=re.findall(r\"[\\w']+\" ,text[2].lower())) ).toDF()\n",
    "\n",
    "AllTokens_df.registerTempTable(\"allTokens\")\n",
    "# AllTokens_df.printSchema()\n",
    "AllTokens_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many words per document?\n",
    "Find out if we have documents that seem too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select min(sz) minimum, avg(sz) average, max(sz) maximum\n",
    "  from (\n",
    "    select size(content) sz\n",
    "    from allTokens\n",
    "  )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select sz, count(sz) cnt\n",
    "  from (\n",
    "    select size(content) sz\n",
    "    from allTokens\n",
    "  )\n",
    "  group by sz\n",
    "  order by sz\n",
    "  limit 20\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are there documents that includes so few words?\n",
    "spark.sql(\"\"\"\n",
    "  select filename, content\n",
    "  from allTokens\n",
    "  where size(content) < 35\n",
    "  \"\"\").take(4)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllTokens_df = spark.sql(\"\"\"\n",
    "  select *\n",
    "  from allTokens\n",
    "  where size(content) > 34\n",
    "\"\"\")\n",
    "AllTokens_df.registerTempTable(\"allTokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data (TF/IDF)\n",
    "We want to prepare the data before splitting it in train and test:\n",
    "- Hashing: convert the words to numbers\n",
    "- TF: Term frequency. How often a word is found in a document\n",
    "- IDF: Inverse document frequency. How many documents a term is found in\n",
    "\n",
    "If we use the tf-idf values, we don't need to remove the stop words since they will likely be removed in the process.\n",
    "\n",
    "The HashingTF class converts words into number using a hashing algorithm and returns a sparse vector \n",
    "that lists the words and count.<br/>\n",
    "There is one vector per document.\n",
    "\n",
    "We use a number of words (features) of 10,007 (prime number) even though we have 8,074 distinct words.\n",
    "This way, there is less chance of collisions in hash values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(numFeatures=10007, inputCol=\"content\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(AllTokens_df).select(\"fileclass\", \"filename\", \"rawFeatures\")\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData).select(\"fileclass\", \"filename\", \"features\")\n",
    "rescaledData.registerTempTable(\"rescaled\")\n",
    "rescaledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the hashing result\n",
    "featurizedData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at IDF result\n",
    "rescaledData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original set in a training and test sets, using an 80 / 20 rule.\n",
    "(NaiveBayesTrain,NaiveBayesTest) = rescaledData.randomSplit([0.8, 0.2], seed = 23)\n",
    "NaiveBayesTrain.registerTempTable(\"train\")\n",
    "NaiveBayesTest.registerTempTable(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number ot training documents: \" + str(NaiveBayesTrain.count()) )\n",
    "print(\"Number ot testing documents: \" + str(NaiveBayesTest.count()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the classes distribution in train and test\n",
    "spark.sql(\"\"\"\n",
    "  select art.fileclass fileclass, art.cnt total, tr.cnt train, te.cnt test\n",
    "  from (\n",
    "     select fileclass, count(filename) as cnt \n",
    "     from rescaled \n",
    "     group by fileclass \n",
    "     order by fileclass\n",
    "   ) art,\n",
    "   (\n",
    "     select fileclass, count(filename) as cnt \n",
    "     from train \n",
    "     group by fileclass \n",
    "     order by fileclass\n",
    "   ) tr,   \n",
    "   (\n",
    "     select fileclass, count(filename) as cnt \n",
    "     from test \n",
    "     group by fileclass \n",
    "     order by fileclass   \n",
    "   ) te\n",
    "   where art.fileclass = tr.fileclass\n",
    "   and   art.fileclass = te.fileclass\n",
    "   order by art.fileclass\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol='features',labelCol='fileclass',modelType=\"multinomial\")\n",
    "model = nb.fit(NaiveBayesTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(NaiveBayesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good is the classification?\n",
    "If the documents were evenly distributed over the 10 categories, a random choice would possibly give\n",
    "us a 10% accuracy.\n",
    "\n",
    "Considering that we have a total of 572 documents and 199 of them are in **wheat**, if we were\n",
    "to simply always say the class is **wheat**, we would get 34.8% accuracy.\n",
    "\n",
    "So, to be successful, the model needs to do better than 34.8% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"fileclass\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More details\n",
    "For this, we use the RDD interface.\n",
    "\n",
    "We can get the precision for each class andthe confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics = MulticlassMetrics(predictions.select(\"fileclass\", \"prediction\").rdd.map(lambda v: (float(v[1]), float(v[0]))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rclassLookupMap=dict()\n",
    "for x in classLookupMap.items() :\n",
    "    rclassLookupMap[x[1]]=x[0]\n",
    "\n",
    "for i in range(10) :\n",
    "    print(rclassLookupMap[i] + \": \" + str(metrics.precision(i)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted classes are in columns, they are ordered by class label ascending\n",
    "# The vertical values can be used to calculate the precision of a specific class\n",
    "arr = metrics.confusionMatrix().toArray()\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\" ALT=\"BSDS Banner\" WIDTH=1195 HEIGHT=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accessing Socrata Open Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 020-Socrata Datasets\n",
    "Execute the next cell if you want to see the `Byte Size Data Science` youtube channel video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.youtube.com/embed/4C9ShcU--ek?rel=0&amp;controls=0&amp;showinfo=0\", width=560, height=315)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library used to read datasets\n",
    "!pip install sodapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed in the notebook\n",
    "import urllib3, requests, json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the Catalog\n",
    "Search for specific datasets and put the result in a Spark dataframe\n",
    "\n",
    "For this example, we search the city of Chicago for transportation datasets.\n",
    "\n",
    "The data coming back is a json document that is quite elaborate. Spark does not seem to be able to hande it properly so we create a schema for it to help in the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "classification = StructType([StructField('categories', ArrayType(StringType()), True),\n",
    "                            StructField('domain_category',StringType(), True),\n",
    "                            StructField('domain_metadata',ArrayType(MapType(StringType(), StringType()), True),True),\n",
    "                            StructField('domain_tags',ArrayType(StringType()), True),\n",
    "                            StructField('tags',ArrayType(StringType()), True)\n",
    "                            ])\n",
    "\n",
    "metadata = StructType([StructField('domain',StringType(), True),\n",
    "                       StructField('license',StringType(), True)\n",
    "                      ])\n",
    "\n",
    "owner = StructType([StructField('display_name',StringType(), True),\n",
    "                    StructField('id',StringType(), True)\n",
    "                   ])\n",
    "\n",
    "resource = StructType([StructField('attribution',StringType(), True),\n",
    "                       StructField('columns_datatype',ArrayType(StringType()), True),\n",
    "                       StructField('columns_description',ArrayType(StringType()), True),\n",
    "                       StructField('columns_field_name',ArrayType(StringType()), True),\n",
    "                       StructField('columns_format',ArrayType(StringType()), True),\n",
    "                       StructField('columns_name',ArrayType(StringType()), True),\n",
    "                       StructField('createdAt',StringType(), True),\n",
    "                       StructField('description',StringType(), True),\n",
    "                       StructField('download_count',LongType(), True),\n",
    "                       StructField('id',StringType(), True),\n",
    "                       StructField('name',StringType(), True),\n",
    "                       StructField('page_views',MapType(StringType(), StringType()), True),\n",
    "                       StructField('parent_fxf',StringType(), True),\n",
    "                       StructField('provenance',StringType(), True),\n",
    "                       StructField('type',StringType(), True),\n",
    "                       StructField('updatedAt',StringType(), True)\n",
    "                          ])\n",
    "\n",
    "# The schema uses the above structures\n",
    "catalogSchema = StructType([StructField('classification', classification, True),\n",
    "                            StructField('link',StringType(), True),\n",
    "                            StructField('metadata', metadata, True),\n",
    "                            StructField('owner', owner,True),\n",
    "                            StructField('permalink',StringType(), True),\n",
    "                            # StructField('published_copy',published_copy, True),\n",
    "                            StructField('resource', resource, True)\n",
    "                           ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, it returns 100 records.\n",
    "# We can get more by using pagination parameters: offset and limit (up to 10000 records)\n",
    "# If there are more, we have to use the scroll_id parameter\n",
    "# the ID of the last result in the previously fetched chunk of results.\n",
    "\n",
    "# In this example we should never go above a few hundreds, probably a lot less.\n",
    "# We still act as if we get lots of returned values\n",
    "\n",
    "url=\"http://api.us.socrata.com/api/catalog/v1\"\n",
    "# Retrieve only datasets\n",
    "urldatasets = url + \"?only=dataset&domains=data.cityofchicago.org\" + \\\n",
    "                    \"&search_context=data.cityofchicago.org\" + \\\n",
    "                    \"&categories=Transportation\"\n",
    "offset=0\n",
    "limit=10000\n",
    "limit2=100\n",
    "done = 0\n",
    "scroll_id=\"\"\n",
    "all_records = dict(results=[])\n",
    "while (done == 0) :\n",
    "    page = \"&scroll_id=\" + scroll_id + \"&limit=\" + str(limit)\n",
    "    # print(url + page)\n",
    "    response = requests.get(urldatasets + page)\n",
    "    if response.status_code != 200 :\n",
    "        print(response.status_code)\n",
    "        done = 1\n",
    "        break\n",
    "    if (offset == 0) :\n",
    "        jsondoc = json.loads(response.text)\n",
    "        scroll_id = jsondoc['results'][len(jsondoc['results']) - 1]['resource']['id']\n",
    "        max_records = jsondoc['resultSetSize']\n",
    "        for val in jsondoc['results'] :\n",
    "            all_records['results'].append(val)\n",
    "    else :\n",
    "        jsondoc = json.loads(response.text)\n",
    "        scroll_id = jsondoc['results'][len(jsondoc['results']) - 1]['resource']['id']\n",
    "        for val in jsondoc['results'] :\n",
    "            all_records['results'].append(val)\n",
    "\n",
    "    offset += limit\n",
    "    if (offset >= max_records) :\n",
    "        done = 1\n",
    "\n",
    "catalog_df = spark.createDataFrame(all_records['results'], schema=catalogSchema )\n",
    "catalog_df.createOrReplaceTempView('socrataCatalog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a quick summary of the records\n",
    "spark.sql(\"\"\"\n",
    "      select metadata.domain,resource.id, substring(resource.updatedAt, 0, 10) updatedAt, trim(resource.name) name\n",
    "      from socrataCatalog\n",
    "      \"\"\").show(n=40,truncate=70,vertical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a dataset\n",
    "Since we have a URL, we should be able to read the dataset using the Requests method.\n",
    "\n",
    "Socrates provides the SODA API (Socrates Open Data Api) to read the data.<br/>\n",
    "The documentation is available at: https://github.com/xmunoz/sodapy\n",
    "\n",
    "See also: https://dev.socrata.com/consumers/getting-started.html\n",
    "\n",
    "The SODA API limits the number of records returned to 50000.\n",
    "you can use the offset and limit parameters to page through the records and read them all.\n",
    "\n",
    "The offset could potentially be used to read new records to update a repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library used to read datasets\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset metadata\n",
    "The API allows us to retrieve a dataset metadata. Looking at the metadata, we find statistics for every column of the dataset. Since it has both a non-null value count and a null value count, we can add both and know how many records there are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofchicago.org\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get statistics on the taxi trips by using its ID\n",
    "tripsMetadata = client.get_metadata(\"wrvz-psew\")\n",
    "\n",
    "# number of records in the dataset\n",
    "taxiRecords = int(tripsMetadata['columns'][0]['cachedContents']['not_null']) + \\\n",
    "              int(tripsMetadata['columns'][0]['cachedContents']['null'])\n",
    "print(\"Taxi records: \" + str(taxiRecords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tripsMetadata['columns'] :\n",
    "    print(col['fieldName'] + \": \" + col['dataTypeName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't create a schema based on the metadata since the fields are all strings\n",
    "# If we could, we could do it this way:\n",
    "taxiSchema = StructType()\n",
    "for col in tripsMetadata['columns'] :\n",
    "    if (col['dataTypeName'] == 'number' or col['dataTypeName'] == 'money') :\n",
    "        taxiSchema.add(StructField(col['fieldName'], FloatType(), True))\n",
    "    else :\n",
    "        taxiSchema.add(StructField(col['fieldName'], StringType(), True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large datasets\n",
    "In some cases, we could run into some memory issues if we collect all the records before creating a dataframe. It is possible to create multiple dataframes and merge them together using the `union` method.\n",
    "\n",
    "For our purpose here, we only get a small subset of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Results returned as JSON from API / converted to Python list of dictionaries by sodapy.\n",
    "# Getting the last 1000 records times out!\n",
    "\n",
    "result_list=[]\n",
    "#offset = taxiRecords - 1000\n",
    "offset = 0\n",
    "\n",
    "# results = client.get(\"wrvz-psew\", offset=offset, limit=1000)   # a large offset causes a timeout\n",
    "# results = client.get(\"wrvz-psew\", where=\"trip_start_timestamp > '2017-07'\", limit=1000)   # Taxi trips\n",
    "results = client.get(\"wrvz-psew\",where=\"trip_start_timestamp > '2017-07'\",order=\"trip_start_timestamp DESC\",limit=1000)   # Taxi trips\n",
    "for res in results :\n",
    "    result_list.append(res)\n",
    "\n",
    "# Create a schema for the taxi data\n",
    "taxiSchema = StructType()\n",
    "for key in result_list[0].keys() :\n",
    "    taxiSchema.add(StructField(key, StringType(), True))\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "results_df = spark.createDataFrame(result_list,taxiSchema)\n",
    "results_df.createOrReplaceTempView('datasetTable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select * from datasetTable\n",
    "  order by trip_start_timestamp desc\n",
    "  limit(10)\n",
    "\"\"\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting columns\n",
    "Most columns don't have the proper type.\n",
    "\n",
    "We could have looked at the data first and created a schema accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert columns to the proper types\n",
    "# This would make it easier for queries\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import column\n",
    "\n",
    "results2_df = results_df.withColumn(\"dropoff_centroid_longitude\", column(\"dropoff_centroid_longitude\").cast(\"double\")).\\\n",
    "                        withColumn(\"pickup_community_area\", column(\"pickup_community_area\").cast(\"int\")).\\\n",
    "                        withColumn(\"dropoff_community_area\", column(\"dropoff_community_area\").cast(\"int\")).\\\n",
    "                        withColumn(\"trip_start_timestamp\", to_timestamp(\"trip_start_timestamp\", \"yyyy-MM-dd'T'HH:mm:ss.SSS\")).\\\n",
    "                        withColumn(\"trip_seconds\", column(\"trip_seconds\").cast(\"int\")).\\\n",
    "                        withColumn(\"dropoff_centroid_latitude\", column(\"dropoff_centroid_latitude\").cast(\"double\")).\\\n",
    "                        withColumn(\"pickup_centroid_longitude\", column(\"pickup_centroid_longitude\").cast(\"double\")).\\\n",
    "                        withColumn(\"tips\", column(\"tips\").cast(\"double\")).\\\n",
    "                        withColumn(\"tolls\", column(\"tolls\").cast(\"double\")).\\\n",
    "                        withColumn(\"trip_end_timestamp\", to_timestamp(\"trip_end_timestamp\", \"yyyy-MM-dd'T'HH:mm:ss.SSS\")).\\\n",
    "                        withColumn(\"trip_miles\", column(\"trip_miles\").cast(\"double\")).\\\n",
    "                        withColumn(\"pickup_centroid_latitude\", column(\"pickup_centroid_latitude\").cast(\"double\")).\\\n",
    "                        withColumn(\"fare\", column(\"fare\").cast(\"double\")).\\\n",
    "                        withColumn(\"trip_total\", column(\"trip_total\").cast(\"double\"))\n",
    "# Show one row to make sure it converted properly. The tricky ones are the timestamps\n",
    "results2_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\" ALT=\"BSDS Banner\" WIDTH=1195 HEIGHT=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# OpenCV Contour Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 043-OpenCVContourDetection\n",
    "Execute the next cell if you want to see the `Byte Size Data Science` youtube channel video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.youtube.com/embed/Wp4xymU6E3Q?rel=0&amp;controls=0&amp;showinfo=0\", width=560, height=315)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the image file we'll be using\n",
    "import sys\n",
    "import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "img_name = 'BSDSImage.png'\n",
    "extension='.jpg'\n",
    "\n",
    "url = 'https://github.com/jacquesroy/byte-size-data-science/raw/master/data/' + img_name\n",
    "# filename = url.rsplit('/', 1)[-1]\n",
    "urllib.request.urlretrieve(url, img_name)\n",
    "\n",
    "%ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the image\n",
    "The image we are uzing has a  width of 864 and a height: 480<br/>\n",
    "The OpenCV library expects that the width and height are multiples of 32.\n",
    "\n",
    "- Width : 864 --> 27 * 32\n",
    "- Height: 480 --> 15 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "img = plt.imread(img_name)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the model weights\n",
    "This particular model is trained on COCO dataset (common objects in context) from Microsoft.\n",
    "It is capable of detecting 80 common objects:\n",
    "\n",
    "airplane, apple, backpack, banana, baseball bat, baseball glove, bear, bed, bench, bicycle, bird, boat, book, bottle, bowl, broccoli, bus, \n",
    "cake, car, carrot, cat, cell phone, chair, clock, couch, cow, cup, dining table, dog, donut, elephant, fire hydrant, fork, frisbee, giraffe, \n",
    "hair drier, handbag, horse, hot dog, keyboard, kite, knife, laptop, microwave, motorcycle, mouse, orange, oven, parking meter, person, \n",
    "pizza, potted plant, refrigerator, remote, sandwich, scissors, sheep, sink, skateboard, skis, snowboard, spoon, sports ball, stop sign, \n",
    "suitcase, surfboard, teddy bear, tennis racket, tie, toaster, toilet, toothbrush, traffic light, train, truck, tv, umbrella, vase, wine glass, \n",
    "zebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm yolov3.weights yolov3.cfg\n",
    "!wget https://pjreddie.com/media/files/yolov3.weights\n",
    "!wget https://github.com/pjreddie/darknet/raw/master/cfg/yolov3.cfg\n",
    "!ls -l yolo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\",\n",
    "\"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\",\n",
    "\"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\",\n",
    "\"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
    "\"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "\"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \n",
    "\"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
    "\"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\",\n",
    "\"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\",\n",
    "\"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\",\n",
    "\"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
    "\"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "\"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
    "\"hair drier\", \"toothbrush\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the color image\n",
    "# options: IMREAD_COLOR (1), IMREAD_GRAYSCALE (0), IMREAD_UNCHANGED (-1)\n",
    "img = cv2.imread(img_name, cv2.IMREAD_COLOR) # Returns None on bad file\n",
    "dims = img.shape\n",
    "print(\"Image width: {}, height: {}, depth: {}\".format(dims[1], dims[0], dims[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "We instantiate the model from the files loader earlier: yolov3.weights yolov3.cfg\n",
    "\n",
    "Then we take a look at some attributes of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the image and extract objects\n",
    "We convert the img numpy array of uint8 and shape (576, 768, 3) to another numpy array of float32 and shape (1, 3, 576, 768).\n",
    "We then set that blob numpy array as the input to our model.\n",
    "\n",
    "The scale value is a multiplier for the values in the array. This way, all the values should be smaller or equal to one. Small values are better for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1./255\n",
    "dims = img.shape\n",
    "# blobFromImage(image, scale, (Width,Height), (0,0,0), True, crop=False)\n",
    "blob = cv2.dnn.blobFromImage(img, scale, (dims[1], dims[0]), (0,0,0), True, crop=False)\n",
    "\n",
    "# Set the input to the model\n",
    "net.setInput(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layers(net):\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return output_layers\n",
    "\n",
    "# function to draw bounding box on the detected object with class name\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "    label = str(classes[class_id])\n",
    "    color1 = np.array([0.0,0.0,255.]) # red\n",
    "    color2 = np.array([0.0,255.0,255.0])# Other yellow\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color1, 2)\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference through the network and gather predictions from output layers\n",
    "outs = net.forward(get_output_layers(net))\n",
    "# input image shape (dims=img.shape)\n",
    "Width=dims[1]\n",
    "Height=dims[0]\n",
    "\n",
    "# initialization\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "# for each detection from each output layer, get the confidence, class id, bounding box params\n",
    "# and ignore weak detections (confidence < 0.5)\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > conf_threshold:\n",
    "            center_x = int(detection[0] * Width)\n",
    "            center_y = int(detection[1] * Height)\n",
    "            w = int(detection[2] * Width)\n",
    "            h = int(detection[3] * Height)\n",
    "            x = center_x - w / 2\n",
    "            y = center_y - h / 2\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes found: \" + str(class_ids))\n",
    "print(\"Classes names: \" + str([classes[i] for i in class_ids]) )\n",
    "print(\"Classes confidence: \" + str(confidences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply non-max suppression\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "# go through the detections remaining\n",
    "# after nms and draw bounding box\n",
    "for i in indices:\n",
    "    i = i[0]\n",
    "    box = boxes[i]\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    \n",
    "    draw_bounding_box(img, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "\n",
    "# save output image to disk\n",
    "retval = cv2.imwrite(\"object-detection\" + extension, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the image with a bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#fig = plt.figure(figsize=(10, 10))\n",
    "# img_plt = plt.imread('object-detection' + extension)\n",
    "#plt.axis('off')\n",
    "#plt.title('image')\n",
    "#plt.imshow(img_plt)\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[0].figure.set_size_inches(18,10)\n",
    "ax[1].figure.set_size_inches(18,10)\n",
    "\n",
    "img0_plt = plt.imread(img_name)\n",
    "img_plt = plt.imread('object-detection' + extension)\n",
    "ax[0].imshow(img0_plt);\n",
    "ax[1].imshow(img_plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class 1 person\n",
    "rect=boxes[0]\n",
    "rect=(int(boxes[0][0]),int(boxes[0][1]),int(boxes[0][2]),int(boxes[0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the original image and the foreground extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(img.shape[:2],np.uint8)\n",
    "bgdModel = np.zeros((1,65),np.float64)\n",
    "fgdModel = np.zeros((1,65),np.float64)\n",
    "\n",
    "# Re-read the original image so we can apply the grabcut on the original \n",
    "# and not the one with the red box\n",
    "img0 = cv2.imread(img_name, cv2.IMREAD_COLOR)\n",
    "\n",
    "cv2.grabCut(img0,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "\n",
    "img2 = img*mask2[:,:,np.newaxis]\n",
    "retval = cv2.imwrite(\"foreground\" + extension, img2)\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[0].figure.set_size_inches(18,10)\n",
    "ax[1].figure.set_size_inches(18,10)\n",
    "\n",
    "img0_plt = plt.imread(img_name)\n",
    "img2_plt = plt.imread(\"foreground\" + extension)\n",
    "ax[0].imshow(img0_plt);\n",
    "ax[1].imshow(img2_plt);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"img2 shape: \" + str(img2.shape))\n",
    "print(\"img2_plt shape: \" + str(img2_plt.shape)) # from a jpeg file\n",
    "print(\"img0_plt shape: \" + str(img0_plt.shape)) # from a png file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread(\"foreground\" + extension, cv2.IMREAD_COLOR)\n",
    "# gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "edges = cv2.Canny(img2,100,200)\n",
    "retval = cv2.imwrite(\"edges\" + extension, edges)\n",
    "edges_plt = plt.imread(\"edges\" + extension)\n",
    "\n",
    "#fig = plt.figure(figsize=(10, 10))\n",
    "# img = plt.imread(img_name)\n",
    "#plt.axis('off')\n",
    "#plt.imshow(edges0_plt)\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[0].figure.set_size_inches(18,10)\n",
    "ax[1].figure.set_size_inches(18,10)\n",
    "\n",
    "img0_plt = plt.imread(img_name)\n",
    "img2_plt = plt.imread(\"foreground\" + extension)\n",
    "ax[0].imshow(img2_plt);\n",
    "ax[1].imshow(edges_plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"img2 shape: \" + str(img2.shape))\n",
    "# print(\"gray shape: \" + str(gray.shape))\n",
    "print(\"edges shape: \" + str(edges.shape))\n",
    "print(\"edges_plt shape: \" + str(edges_plt.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the contour image bigger\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.axis('off')\n",
    "plt.imshow(edges_plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.jpg *.png yolo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

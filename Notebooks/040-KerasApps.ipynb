{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras Application: Visual Recognition\n",
    "Try some visual recognition Keras application\n",
    "\n",
    "see also: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files downloaded are used in this notebook.<br/>\n",
    "The three compressed tar files split the data since github accepts maximum file sizes of 25MB and the three together were 28MB in size.\n",
    "\n",
    "The file name in `img_path` is the file that will be classified. This way, we could easily change the file for another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download files to the workspace.\n",
    "import sys\n",
    "import types\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "img_path = 'elantra.jpg'\n",
    "\n",
    "url = 'https://github.com/jacquesroy/byte-size-data-science/raw/master/data/' + img_path\n",
    "# filename = url.rsplit('/', 1)[-1]\n",
    "urllib.request.urlretrieve(url, img_path)\n",
    "url = 'https://github.com/jacquesroy/byte-size-data-science/raw/master/data/cardatatrain.tgz'\n",
    "urllib.request.urlretrieve(url, 'cardatatrain.tgz')\n",
    "url = 'https://github.com/jacquesroy/byte-size-data-science/raw/master/data/cardatatest.tgz'\n",
    "urllib.request.urlretrieve(url, 'cardatatest.tgz')\n",
    "url = 'https://github.com/jacquesroy/byte-size-data-science/raw/master/data/cardatavalid.tgz'\n",
    "urllib.request.urlretrieve(url, 'cardatavalid.tgz')\n",
    "!tar xzf cardatatrain.tgz\n",
    "!tar xzf cardatatest.tgz\n",
    "!tar xzf cardatavalid.tgz\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "img = plt.imread('elantra.jpg')\n",
    "plt.axis('off')\n",
    "plt.title('image')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications as app\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify ImageNet classes with ResNet50\n",
    "ResNet50 is a convolutional neural network that is trained on more than a million images from the ImageNet database. The network is 50 layers deep and can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "# from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "nbweights = 0\n",
    "for m in model.get_weights() :\n",
    "    nb = 1\n",
    "    for v in m.shape : # multiply the multiple dimensions together to get the real total\n",
    "        nb = nb * v\n",
    "    nbweights = nbweights + nb\n",
    "\n",
    "print(\"Number of layers: \" + str(len(model.layers)))\n",
    "print(\"Number of weights: \" + str(nbweights) + \", size: \" + str(nbweights * 4))\n",
    "print(\"Number of output values: \" + str(model.layers[-1].output_shape[1]))\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = app.resnet50.preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', app.resnet50.decode_predictions(preds, top=5)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception\n",
    "Xception is a convolutional neural network that is trained on more than a million images from the ImageNet database. \n",
    "\n",
    "Xception is an extension of the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception\n",
    "\n",
    "#model = Xception(include_top=False, weights='imagenet', input_tensor=None, input_shape=(1600, 1200, 3), pooling=None, classes=1000)\n",
    "model = Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "\n",
    "nbweights = 0\n",
    "for m in model.get_weights() :\n",
    "    nb = 1\n",
    "    for v in m.shape : # multiply the multiple dimensions together to get the real total\n",
    "        nb = nb * v\n",
    "    nbweights = nbweights + nb\n",
    "\n",
    "print(\"Number of layers: \" + str(len(model.layers)))\n",
    "print(\"Number of weights: \" + str(nbweights) + \", size: \" + str(nbweights * 4))\n",
    "print(\"Number of output values: \" + str(model.layers[-1].output_shape[1]))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(img_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = app.xception.preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', app.xception.decode_predictions(preds, top=3)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features with VGG16\n",
    "VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. It was used to win the ILSVR (ImageNet) competition in 2014.\n",
    "\n",
    "Feature extraction relates to using the base part of a model to process images before passing them through another model\n",
    "that trained for further classification.\n",
    "\n",
    "A good reference is:<br/>\n",
    "\"Deep Learning with Python\"<br/>\n",
    "Fran√ßois Chollet<br/>\n",
    "November 2017<br/>\n",
    "ISBN 9781617294433  (384 pages)<br/>\n",
    "(companion site: https://github.com/fchollet/deep-learning-with-python-notebooks)\n",
    "\n",
    "see also: https://medium.com/@franky07724_57962/using-keras-pre-trained-models-for-feature-extraction-in-image-clustering-a142c6cdf5b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a VGG16 model on top of another one\n",
    "Use the base model from VGG16 and add additional layers for the classification.<br/>\n",
    "This saves time and effort since the feature extraction is already trained, we only need to specialize it with additional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to check if the table shown in the video has the right depth...\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True) # Use it for classification \n",
    "nbweights = 0\n",
    "for m in model.get_weights() :\n",
    "    nb = 1\n",
    "    for v in m.shape : # multiply the multiple dimensions together to get the real total\n",
    "        nb = nb * v\n",
    "    nbweights = nbweights + nb\n",
    "\n",
    "print(\"Number of layers: \" + str(len(model.layers)))\n",
    "print(\"Number of weights: \" + str(nbweights) + \", size: \" + str(nbweights * 4))\n",
    "print(\"Output shape: \" + str(model.layers[-1].output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "# from keras.preprocessing import image\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "# import numpy as np\n",
    "\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = app.vgg16.preprocess_input(x)\n",
    "\n",
    "nbweights = 0\n",
    "for m in model.get_weights() :\n",
    "    nb = 1\n",
    "    for v in m.shape : # multiply the multiple dimensions together to get the real total\n",
    "        nb = nb * v\n",
    "    nbweights = nbweights + nb\n",
    "\n",
    "print(\"Number of layers: \" + str(len(vgg16_model.layers)))\n",
    "print(\"Number of weights: \" + str(nbweights) + \", size: \" + str(nbweights * 4))\n",
    "print(\"Output shape: \" + str(vgg16_model.layers[-1].output_shape))\n",
    "\n",
    "vgg16_model.summary()\n",
    "\n",
    "# features shape: (1, 7, 7, 512) if include_top is False. Otherwise, prediction\n",
    "car_features = vgg16_model.predict(x)\n",
    "\n",
    "# When include_top is True, we get a prediction over 1000 categories\n",
    "# print('Predicted:', app.vgg16.decode_predictions(features, top=3)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep images\n",
    "# See: https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb\n",
    "# and: https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = 'data/train'\n",
    "validation_dir = 'data/valid'\n",
    "test_dir = 'data/test'\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 2\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 512))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    class_dictionary = generator.class_indices\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = vgg16_model.predict(inputs_batch) # model is the VGG16 above\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels, class_dictionary\n",
    "\n",
    "train_features, train_labels, train_classes_dict = extract_features(train_dir, 131)\n",
    "validation_features, validation_labels, validation_classes_dict = extract_features(validation_dir, 64)\n",
    "test_features, test_labels, test_classes_dict = extract_features(test_dir, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what are the classes and their numerical value/position\n",
    "print(train_classes_dict)\n",
    "print(validation_classes_dict)\n",
    "print(test_classes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the features\n",
    "Flatten the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (131, 7 * 7 * 512))\n",
    "validation_features = np.reshape(validation_features, (64, 7 * 7 * 512))\n",
    "test_features = np.reshape(test_features, (65, 7 * 7 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=7 * 7 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=25,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first number BMW, second elantra, and third Volvo\n",
    "print(train_classes_dict)\n",
    "model.predict(np.reshape(car_features, (1, 7 * 7 * 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

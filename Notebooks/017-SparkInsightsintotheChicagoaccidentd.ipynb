{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\" ALT=\"BSDS Banner\" WIDTH=1195 HEIGHT=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Chicago Car Accident Data Analysis\n",
    "In this notebook, we analyze the data using a Python environment.<br/>\n",
    "We also use Pixiedust as the engine over Mapbox to display maps in the later part of the analysis.\n",
    "\n",
    "In an additional section, we see how we could use additional data to add the city name to each record.\n",
    "\n",
    "## Additional Information\n",
    "The chicago accident information includes three files: Crashes, people, and vehicles.\n",
    "\n",
    "In this notebook, we explore the crashes through a file called `ChicagoTrafficCrashes20180917.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 017-Spark Data Exploration \n",
    "Execute the next cell if you want to see the `Byte Size Data Science` youtube channel video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.youtube.com/embed/xSDP6u_Xqhc?rel=0&amp;controls=0&amp;showinfo=0\", width=560, height=315)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the crash data\n",
    "In this section, we read the data as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PixieDust is an open source library that was contributed by IBM\n",
    "!pip install --user --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/jacquesroy/byte-size-data-science/raw/master/data/ChicagoTrafficCrashes20180917.csv.zip'\n",
    "# get the filename from the url: \"ChicagoTrafficCrashes20180917.csv\"\n",
    "zipfilename = url.rsplit('/', 1)[-1]\n",
    "filename = zipfilename.rsplit('.', 1)[0]\n",
    "print (\"zipfilename: \" + zipfilename)\n",
    "print(\"filename: \" +filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(url, zipfilename)\n",
    "compressed_file = zipfile.ZipFile(zipfilename)\n",
    "csv_file = compressed_file.extract(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions_df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load(filename)\n",
    "\n",
    "collisions_df.createOrReplaceTempView(\"collisions\")\n",
    "spark.sql(\"\"\"\n",
    "    select rd_no,crash_date,DATE_POLICE_NOTIFIED, LATITUDE,LONGITUDE\n",
    "    from   collisions\n",
    "    limit 5\n",
    "    \"\"\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of records and display the DataFrame schema\n",
    "print(\"Records: {}\".format(collisions_df.count()))\n",
    "collisions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the two datetime columns to the proper type\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import column\n",
    "\n",
    "collisions_df = collisions_df.withColumn(\"CRASH_TS\", to_timestamp(\"CRASH_DATE\", \"MM/dd/yyyy hh:mm:ss aa\")).\\\n",
    "                              withColumn(\"CRASH_DATE\", column('CRASH_TS').cast('date')).\\\n",
    "                              withColumn(\"DATE_POLICE_NOTIFIED\", to_timestamp(\"DATE_POLICE_NOTIFIED\", \"MM/dd/yyyy hh:mm:ss aa\"))\n",
    "collisions_df.createOrReplaceTempView(\"collisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get statistics for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = collisions_df.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multiple cells to show the results in a readable manner\n",
    "stats_df.select(['summary','RD_NO','CRASH_DATE_EST_I','POSTED_SPEED_LIMIT','TRAFFIC_CONTROL_DEVICE','DEVICE_CONDITION']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous output, we see that `CRASH_DATE_EST_I` is mostly null (92.5% of the time)<br/>\n",
    "We see that the `POSTED_SPEED_LIMIT maximum` is 99 so that raises some questions.<br/>\n",
    "The last two columns are likely from a short list of possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.select(['summary','WEATHER_CONDITION','LIGHTING_CONDITION','FIRST_CRASH_TYPE','TRAFFICWAY_TYPE','LANE_CNT','ALIGNMENT']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have five string columns that are list of possibilities.<br/>\n",
    "Since we had two other columns earlier, let's find out how many possibilities in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select count(distinct TRAFFIC_CONTROL_DEVICE) TRAFFIC_CONTROL_DEVICE, count(distinct DEVICE_CONDITION) DEVICE_CONDITION,\n",
    "         count(distinct WEATHER_CONDITION) WEATHER_CONDITION,\n",
    "         count(distinct LIGHTING_CONDITION) LIGHTING_CONDITION, count(distinct FIRST_CRASH_TYPE) FIRST_CRASH_TYPE,\n",
    "         count(distinct TRAFFICWAY_TYPE) TRAFFICWAY_TYPE, count(distinct ALIGNMENT) ALIGNMENT\n",
    "  from collisions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing with column statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.select(['summary','ROADWAY_SURFACE_COND','ROAD_DEFECT','REPORT_TYPE','CRASH_TYPE','INTERSECTION_RELATED_I','NOT_RIGHT_OF_WAY_I']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw a bunch of string columns.<br/>\n",
    "Note that `INTERSECTION_RELATED_I` and `NOT_RIGHT_OF_WAY_I` are often null (80.2% and 95.6% respectively)<br/>\n",
    "Let's look at the other columns to see how many distinct values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select count(distinct ROADWAY_SURFACE_COND) ROADWAY_SURFACE_COND, count(distinct ROAD_DEFECT) ROAD_DEFECT,\n",
    "         count(distinct REPORT_TYPE) REPORT_TYPE, count(distinct CRASH_TYPE) CRASH_TYPE\n",
    "  from collisions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.select(['summary','HIT_AND_RUN_I','DAMAGE','PRIM_CONTRIBUTORY_CAUSE','SEC_CONTRIBUTORY_CAUSE','STREET_DIRECTION']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HIT_AND_RUN_I` is often null (72.6%).<br/>\n",
    "We can look at the choices in the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select count(distinct STREET_NAME) STREET_NAME, count(distinct DAMAGE) DAMAGE, count(distinct PRIM_CONTRIBUTORY_CAUSE) PRIM_CONTRIBUTORY_CAUSE,\n",
    "         count(distinct SEC_CONTRIBUTORY_CAUSE) SEC_CONTRIBUTORY_CAUSE, count(distinct STREET_DIRECTION) STREET_DIRECTION\n",
    "  from collisions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.select(['summary','BEAT_OF_OCCURRENCE','PHOTOS_TAKEN_I','STATEMENTS_TAKEN_I','DOORING_I','WORK_ZONE_I','WORK_ZONE_TYPE']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BEAT_OF_OCCURRENCE` could be useful in terms of resource deployment.<br/>\n",
    "The other columns are null most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.select(['summary','WORKERS_PRESENT_I','NUM_UNITS','MOST_SEVERE_INJURY','INJURIES_TOTAL','INJURIES_FATAL','INJURIES_INCAPACITATING']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WORKERS_PRESENT_I` is mostly null.<br/>\n",
    "The other columns appear \"normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.select(['summary','INJURIES_NON_INCAPACITATING','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_NO_INDICATION','INJURIES_UNKNOWN']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.select(['summary','CRASH_HOUR','CRASH_DAY_OF_WEEK','CRASH_MONTH','LATITUDE','LONGITUDE']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring further\n",
    "We saw earlier that the minumum `CRASH DATE` was 2014-01-21 and the minimum `DATE_POLICE_NOTIFIED` was 2015-07-25<br/>\n",
    "That indicates that there probably are some errors in the data.\n",
    "\n",
    "The `POSTED_SPEED_LIMIT` value has a maximum of 99. This is suspicious.\n",
    "\n",
    "There are more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATE_POLICE_NOTIFIED should always be greater or equal to CRASH_DATE\n",
    "data_df = spark.sql(\"\"\"\n",
    "  select RD_NO, CRASH_DATE, DATE_POLICE_NOTIFIED, datediff(to_date(DATE_POLICE_NOTIFIED), CRASH_DATE) DIFF_DAYS\n",
    "  from collisions\n",
    "\"\"\")\n",
    "data_df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select datediff(to_date(DATE_POLICE_NOTIFIED), CRASH_DATE) DIFF_DAYS, count(*) cnt\n",
    "  from collisions\n",
    "  group by DIFF_DAYS\n",
    "  order by DIFF_DAYS desc\n",
    "  limit 20\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.filter('DIFF_DAYS > 0').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.filter('DIFF_DAYS > 30').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.filter('DIFF_DAYS > 30').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSTED_SPEED_LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select POSTED_SPEED_LIMIT, count(POSTED_SPEED_LIMIT) TOTAL\n",
    "  from collisions\n",
    "  group by POSTED_SPEED_LIMIT\n",
    "  order by POSTED_SPEED_LIMIT Desc\n",
    "\"\"\").show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the distinct values\n",
    "How relevant are the distinct values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select TRAFFIC_CONTROL_DEVICE, count(TRAFFIC_CONTROL_DEVICE) TOTAL\n",
    "  from collisions\n",
    "  group by TRAFFIC_CONTROL_DEVICE\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select DEVICE_CONDITION, count(DEVICE_CONDITION) TOTAL\n",
    "  from collisions\n",
    "  group by DEVICE_CONDITION\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select WEATHER_CONDITION, count(WEATHER_CONDITION) TOTAL\n",
    "  from collisions\n",
    "  group by WEATHER_CONDITION\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select LIGHTING_CONDITION, count(LIGHTING_CONDITION) TOTAL\n",
    "  from collisions\n",
    "  group by LIGHTING_CONDITION\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select FIRST_CRASH_TYPE, count(FIRST_CRASH_TYPE) TOTAL\n",
    "  from collisions\n",
    "  group by FIRST_CRASH_TYPE\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select TRAFFICWAY_TYPE, count(TRAFFICWAY_TYPE) TOTAL\n",
    "  from collisions\n",
    "  group by TRAFFICWAY_TYPE\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select ALIGNMENT, count(ALIGNMENT) TOTAL\n",
    "  from collisions\n",
    "  group by ALIGNMENT\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select ROADWAY_SURFACE_COND, count(ROADWAY_SURFACE_COND) TOTAL\n",
    "  from collisions\n",
    "  group by ROADWAY_SURFACE_COND\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select ROAD_DEFECT, count(ROAD_DEFECT) TOTAL\n",
    "  from collisions\n",
    "  group by ROAD_DEFECT\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select REPORT_TYPE, count(REPORT_TYPE) TOTAL\n",
    "  from collisions\n",
    "  group by REPORT_TYPE\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select CRASH_TYPE, count(CRASH_TYPE) TOTAL\n",
    "  from collisions\n",
    "  group by CRASH_TYPE\n",
    "  order by TOTAL desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count accidents, accidents with injuries, accidents with casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  select count(*) all_accidents from collisions\n",
    "  where longitude is not null\n",
    "  and latitude is not null\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  select count(*) accidents_with_injuries from collisions\n",
    "  where longitude is not null\n",
    "  and latitude is not null\n",
    "  and INJURIES_TOTAL > 0\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  select count(*) accidents_with_fatalities from collisions\n",
    "  where longitude is not null\n",
    "  and latitude is not null\n",
    "  and INJURIES_FATAL > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a subset of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import column\n",
    "# Select the columns to use\n",
    "# RD_NO,CRASH_DATE,POSTED_SPEED_LIMIT,TRAFFIC_CONTROL_DEVICE,DEVICE_CONDITION,\n",
    "#         WEATHER_CONDITION,LIGHTING_CONDITION,FIRST_CRASH_TYPE,TRAFFICWAY_TYPE,LANE_CNT,\n",
    "#         ALIGNMENT,ROADWAY_SURFACE_COND,ROAD_DEFECT,REPORT_TYPE,CRASH_TYPE,\n",
    "#         INTERSECTION_RELATED_I,HIT_AND_RUN_I,DAMAGE,DATE_POLICE_NOTIFIED,\n",
    "#         PRIM_CONTRIBUTORY_CAUSE,SEC_CONTRIBUTORY_CAUSE,STREET_NO,STREET_DIRECTION,\n",
    "#         STREET_NAME,BEAT_OF_OCCURRENCE,NUM_UNITS,MOST_SEVERE_INJURY,INJURIES_TOTAL,\n",
    "#         INJURIES_FATAL,INJURIES_INCAPACITATING,INJURIES_NON_INCAPACITATING,\n",
    "#         INJURIES_REPORTED_NOT_EVIDENT,INJURIES_NO_INDICATION,INJURIES_UNKNOWN,\n",
    "#         CRASH_HOUR,CRASH_DAY_OF_WEEK,CRASH_MONTH,LATITUDE,LONGITUDE\n",
    "#\n",
    "# Additional columns (derived from CRASH_DATE)\n",
    "# CRASH_TS (original CRASH_DATE as a timestamp)\n",
    "# CRASH DATE (derived from CRASH_TS as a date)\n",
    "#\n",
    "collisions2_df = spark.sql(\"\"\"\n",
    "  select RD_NO,CRASH_TS,CRASH_DATE, INJURIES_TOTAL, INJURIES_FATAL,\n",
    "         INJURIES_INCAPACITATING,INJURIES_NON_INCAPACITATING,\n",
    "         INJURIES_REPORTED_NOT_EVIDENT,INJURIES_NO_INDICATION,INJURIES_UNKNOWN,\n",
    "         CRASH_HOUR,CRASH_DAY_OF_WEEK,dayofmonth(CRASH_DATE) CRASH_DAY, \n",
    "         CRASH_MONTH,year(CRASH_DATE) CRASH_YEAR, LATITUDE,LONGITUDE\n",
    "  from collisions\n",
    "\"\"\")\n",
    "collisions2_df.createOrReplaceTempView(\"collisions2\")\n",
    "collisions2_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.patches lets us create colored patches, which we can use for legends in plots\n",
    "import matplotlib.patches as mpatches\n",
    "# seaborn also builds on matplotlib and adds graphical features and new plot types\n",
    "# adjust settings\n",
    "# The inline statement insures that the plot will show in the cell output. Look at the documentation for more information\n",
    "%matplotlib inline\n",
    "sns.set_style(\"white\")\n",
    "plt.rcParams['figure.figsize'] = (15, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping accidents\n",
    "First by street for 3 categories:\n",
    "<ul><li>All accidents</li>\n",
    "<li>Accidents with injuries</li>\n",
    "<li>Accidents with fatalities</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 15 streets by accident count\n",
    "plt.figure(figsize=(8,5))\n",
    "streets = collisions_df.groupBy('STREET_NAME').count().sort('count',ascending=False).limit(15).toPandas() \n",
    "colors = ['g','0.75','y','k','b','r']\n",
    "streets.sort_values(by='count', ascending=False)['count'].plot.barh(color=colors)\n",
    "plt.xlabel('Collisions')\n",
    "plt.ylabel('Street')\n",
    "plt.title('Total Number of Collisions by Street', size=15)\n",
    "plt.yticks(range(0,15),streets['STREET_NAME'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "streets = collisions_df.filter(collisions_df.INJURIES_FATAL == 0).filter(collisions_df.INJURIES_TOTAL > 0).groupBy('STREET_NAME').count().sort('count',ascending=False).limit(15).toPandas() # .iloc[1:,:]\n",
    "colors = ['g','0.75','y','k','b','r']\n",
    "streets.sort_values(by='count', ascending=False)['count'].plot.barh(color=colors)\n",
    "plt.xlabel('Collisions')\n",
    "plt.ylabel('Street')\n",
    "plt.title('Total Number of injuries accidents by Street', size=15)\n",
    "plt.yticks(range(0,15),streets['STREET_NAME'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "streets = collisions_df.filter(collisions_df.INJURIES_FATAL > 0).\\\n",
    "                        groupBy('STREET_NAME').count().sort('count',ascending=False).limit(15).toPandas()\n",
    "colors = ['g','0.75','y','k','b','r']\n",
    "streets.sort_values(by='count', ascending=False)['count'].plot.barh(color=colors)\n",
    "plt.xlabel('Collisions')\n",
    "plt.ylabel('Street')\n",
    "plt.title('Total Number of Fatal accidents by Street', size=15)\n",
    "plt.yticks(range(0,15),streets['STREET_NAME'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accidents by month, day of the week\n",
    "See example code at: `http://benalexkeen.com/bar-charts-in-matplotlib/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "byMonth = collisions_df.groupBy('CRASH_MONTH').count().sort('CRASH_MONTH',ascending=False).toPandas()\n",
    "colors = ['g','0.75','y','k','b','r']\n",
    "byMonth.sort_values(by='CRASH_MONTH', ascending=False)['count'].plot.barh(color=colors)\n",
    "plt.xlabel('Collisions')\n",
    "plt.ylabel('month')\n",
    "plt.title('Total Number of Collisions by month', size=15)\n",
    "plt.yticks(range(0,12),byMonth['CRASH_MONTH'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "byDay = collisions_df.groupBy('CRASH_DAY_OF_WEEK').count().sort('CRASH_DAY_OF_WEEK',ascending=False).toPandas()\n",
    "colors = ['g','0.75','y','k','b','r']\n",
    "byDay.sort_values(by='CRASH_DAY_OF_WEEK', ascending=False)['count'].plot.barh(color=colors)\n",
    "plt.xlabel('Collisions')\n",
    "plt.ylabel('month')\n",
    "plt.title('Total Number of Collisions by Day of the week', size=15)\n",
    "plt.yticks(range(0,7),byDay['CRASH_DAY_OF_WEEK'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "byHour = collisions_df.groupBy('CRASH_HOUR').count().sort('CRASH_HOUR',ascending=False).toPandas()\n",
    "colors = ['g','0.75','y','k','b','r']\n",
    "byHour.sort_values(by='CRASH_HOUR', ascending=False)['count'].plot.barh(color=colors)\n",
    "plt.xlabel('Collisions')\n",
    "plt.ylabel('month')\n",
    "plt.title('Total Number of Collisions by hour of the day', size=15)\n",
    "plt.yticks(range(0,24),byHour['CRASH_HOUR'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions_pd = collisions_df[collisions_df['LATITUDE'] != 0][['LATITUDE', 'LONGITUDE', 'CRASH_DATE',\n",
    "                                                               'INJURIES_TOTAL', 'INJURIES_FATAL', 'CRASH_HOUR','CRASH_DAY_OF_WEEK',\n",
    "                                                               'CRASH_MONTH']].toPandas()\n",
    "\n",
    "collisions_pd.columns = ['Latitude', 'Longitude', 'Date', 'Persons Injured', 'Persons Killed',\n",
    "                         'Crash hour', 'Crash day of week', 'Crash month']\n",
    "\n",
    "collisions_pd['Latitude'] = collisions_pd['Latitude'].astype(float)\n",
    "collisions_pd['Longitude'] = collisions_pd['Longitude'].astype(float)\n",
    "collisions_pd['Persons Killed'] = collisions_pd['Persons Killed'].astype(float)\n",
    "collisions_pd['Persons Injured'] = collisions_pd['Persons Injured'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "#divide dataset into accident categories: fatal, non-fatal but with injuries, none of the above\n",
    "killed_pd = collisions_pd[collisions_pd['Persons Killed']>0]\n",
    "injured_pd = collisions_pd[np.logical_and(collisions_pd['Persons Injured']>0, collisions_pd['Persons Killed']==0)]\n",
    "nothing_pd = collisions_pd[np.logical_and(collisions_pd['Persons Killed']==0, collisions_pd['Persons Injured']==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the accidents using longitude/latitude\n",
    "This is not a map but a graphical representation of the accidents related to longitude and latitude.\n",
    "\n",
    "We got the limits for longitude and latitude earlier and plug them into the xlim/ylim values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create scatterplots\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(collisions_pd.Longitude, collisions_pd.Latitude, alpha=0.05, s=4, color='darkseagreen')\n",
    "\n",
    "#adjust more settings\n",
    "plt.title('Motor Vehicle Collisions in Chicago', size=25)\n",
    "plt.xlim((-87.92,-87.52))\n",
    "plt.ylim((41.64,42.03))\n",
    "plt.xlabel('Longitude',size=20)\n",
    "plt.ylabel('Latitude',size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance the scatter plot to identify the accidents severity\n",
    "We draw from Pandas DataFrames we created earlier to plot the severity in different color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust settings\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "#create scatterplots\n",
    "plt.scatter(nothing_pd.Longitude, nothing_pd.Latitude, alpha=0.04, s=1, color='blue')\n",
    "plt.scatter(injured_pd.Longitude, injured_pd.Latitude, alpha=0.1, s=1, color='yellow')\n",
    "plt.scatter(killed_pd.Longitude, killed_pd.Latitude, color='red', s=5)\n",
    "\n",
    "#create legend\n",
    "blue_patch = mpatches.Patch( label='car body damage', alpha=0.2, color='blue')\n",
    "yellow_patch = mpatches.Patch(color='yellow', label='personal injury', alpha=0.5)\n",
    "red_patch = mpatches.Patch(color='red', label='lethal accidents')\n",
    "plt.legend([blue_patch, yellow_patch, red_patch],('car body damage', 'personal injury', 'fatal accidents'), \n",
    "           loc='upper left', prop={'size':20})\n",
    "\n",
    "#adjust more settings\n",
    "plt.title('Severity of Motor Vehicle Collisions in Chicago', size=20)\n",
    "plt.xlim((-87.92,-87.52))\n",
    "plt.ylim((41.64,42.03))\n",
    "plt.xlabel('Longitude',size=20)\n",
    "plt.ylabel('Latitude',size=20)\n",
    "plt.savefig('anothertry.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the streets with the most collisions\n",
    "Find the top ten streets in New York where the most vehicle collisions occurred. Display the results in a bar graph and as a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Note the Spark DataFrame SQL-like methods available: groupBy, agg, sort (order by), limit\n",
    "# The result is converted to a Pandas DataFrame\n",
    "plottingdf = collisions_df.groupBy(\"STREET_NAME\").agg(F.count(\"STREET_NAME\").alias(\"count(STREET_NAME)\")).\\\n",
    "sort(F.desc('count(STREET_NAME)')).limit(10).toPandas()\n",
    "\n",
    "plottingdf[['count(STREET_NAME)']].plot(kind='barh', figsize=(11,7), legend=False)\n",
    "plt.title('Top 10 Streets with the most accidents', size=20)\n",
    "plt.xlabel('Count')\n",
    "plt.yticks(range(10), plottingdf['STREET_NAME'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the 10 streets with the most collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = collisions_df[['STREET_NAME', 'LATITUDE', 'LONGITUDE']].toPandas()\n",
    "street_names = collisions_df.groupBy(\"STREET_NAME\").agg(F.count(\"STREET_NAME\").\n",
    "                                                        alias(\"count(STREET_NAME)\")).\\\n",
    "                sort(F.desc('count(STREET_NAME)')).limit(10).select('STREET_NAME').\\\n",
    "                rdd.map(lambda r : r.STREET_NAME).collect()\n",
    "\n",
    "collisions1 = data1[data1['STREET_NAME']==street_names[0]]\n",
    "collisions2 = data1[data1['STREET_NAME']==street_names[1]]\n",
    "collisions3 = data1[data1['STREET_NAME']==street_names[2]]\n",
    "collisions4 = data1[data1['STREET_NAME']==street_names[3]]\n",
    "collisions5 = data1[data1['STREET_NAME']==street_names[4]]\n",
    "collisions6 = data1[data1['STREET_NAME']==street_names[5]]\n",
    "collisions7 = data1[data1['STREET_NAME']==street_names[6]]\n",
    "collisions8 = data1[data1['STREET_NAME']==street_names[7]]\n",
    "collisions9 = data1[data1['STREET_NAME']==street_names[8]]\n",
    "collisions10 = data1[data1['STREET_NAME']==street_names[9]]\n",
    "\n",
    "#create scatterplots\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(data1.LONGITUDE, data1.LATITUDE, s=1, color='darkseagreen')\n",
    "plt.scatter(collisions1.LONGITUDE, collisions1.LATITUDE, s=2, color='red')\n",
    "plt.scatter(collisions2.LONGITUDE, collisions2.LATITUDE, color='blue', s=2)\n",
    "plt.scatter(collisions3.LONGITUDE, collisions3.LATITUDE, s=2, color='magenta')\n",
    "plt.scatter(collisions4.LONGITUDE, collisions4.LATITUDE, color='orange', s=2)\n",
    "plt.scatter(collisions5.LONGITUDE, collisions5.LATITUDE, s=2, color='yellow')\n",
    "plt.scatter(collisions6.LONGITUDE, collisions6.LATITUDE, color='purple', s=2)\n",
    "plt.scatter(collisions7.LONGITUDE, collisions7.LATITUDE, s=2, color='black')\n",
    "plt.scatter(collisions8.LONGITUDE, collisions8.LATITUDE, color='chartreuse', s=2)\n",
    "plt.scatter(collisions9.LONGITUDE, collisions9.LATITUDE, s=2, color='brown')\n",
    "plt.scatter(collisions10.LONGITUDE, collisions10.LATITUDE, color='darkgreen', s=2)\n",
    "\n",
    "\n",
    "#create legend\n",
    "a_patch = mpatches.Patch(color='red', label=street_names[0])\n",
    "b_patch = mpatches.Patch(color='blue', label=street_names[1])\n",
    "c_patch = mpatches.Patch(color='magenta', label=street_names[2])\n",
    "d_patch = mpatches.Patch(color='orange', label=street_names[3])\n",
    "e_patch = mpatches.Patch(color='yellow', label=street_names[4])\n",
    "f_patch = mpatches.Patch(color='purple', label=street_names[5])\n",
    "g_patch = mpatches.Patch(color='black', label=street_names[6])\n",
    "h_patch = mpatches.Patch(color='chartreuse', label=street_names[7])\n",
    "i_patch = mpatches.Patch(color='brown', label=street_names[8])\n",
    "j_patch = mpatches.Patch(color='darkgreen', label=street_names[9])\n",
    "\n",
    "plt.legend([a_patch, b_patch, c_patch, d_patch, e_patch, f_patch, g_patch, h_patch, i_patch, j_patch],\n",
    "            (street_names[0],street_names[1],street_names[2],street_names[3],street_names[4],\n",
    "             street_names[5],street_names[6],street_names[7],street_names[8],street_names[9]),\n",
    "           loc='upper left', prop={'size':12})\n",
    "\n",
    "#adjust more settings\n",
    "plt.title('Vehicle Collisions in Chicago', size=25)\n",
    "plt.xlim((-87.92,-87.52))\n",
    "plt.ylim((41.64,42.03))\n",
    "plt.xlabel('Longitude',size=20)\n",
    "plt.ylabel('Latitude',size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using K-Means to find hot spots\n",
    "We are using K-means to find the center of groupings of accidents.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "We extract the longitude and latitude of all accidents\n",
    "We create a model (for, arbitrarily, 10 clusters)\n",
    "We extract the centers and convert them to a Panda DataFrame\n",
    "We display the result on a map using pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column features must be of type org.apache.spark.ml.linalg.Vector.\n",
    "data1 = spark.createDataFrame(\n",
    "    spark.sql(\"\"\"\n",
    "          select LONGITUDE, LATITUDE from collisions\n",
    "          where LATITUDE is not null\n",
    "          and longitude is not null\n",
    "    \"\"\").rdd.map(lambda r : Row(Vectors.dense([r.LONGITUDE, r.LATITUDE]))), [\"features\"] )\n",
    "\n",
    "kmeans = KMeans(k=10, seed=123)\n",
    "model = kmeans.fit(data1)\n",
    "centers = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a Panda DataFrame\n",
    "long=[]\n",
    "lat=[]\n",
    "for center in centers :\n",
    "    long.append(center[0])\n",
    "    lat.append(center[1])\n",
    "\n",
    "summary = model.summary\n",
    "data2 = pd.DataFrame(data={'LONGITUDE': long, 'LATITUDE': lat, \"COUNT\": summary.clusterSizes })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "basemap": "outdoors-v9",
      "coloropacity": "87",
      "colorrampname": "Orange to Purple",
      "handlerId": "mapView",
      "keyFields": "LATITUDE,LONGITUDE",
      "mapboxtoken": "pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4M29iazA2Z2gycXA4N2pmbDZmangifQ.-g_vE53SD2WrJ6tFX7QHmA",
      "rowCount": "100000",
      "title": "10 Centers of Accidents",
      "valueFields": "COUNT"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means for accidents with injuries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = spark.createDataFrame(\n",
    "    spark.sql(\"\"\"\n",
    "          select LONGITUDE, LATITUDE from collisions\n",
    "          where LATITUDE is not null\n",
    "          and longitude is not null\n",
    "          and INJURIES_TOTAL > 0\n",
    "    \"\"\").rdd.map(lambda r : Row(Vectors.dense([r.LONGITUDE, r.LATITUDE]))), [\"features\"] )\n",
    "\n",
    "kmeans = KMeans(k=100, seed=123)\n",
    "model = kmeans.fit(data1)\n",
    "centers = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "basemap": "outdoors-v9",
      "coloropacity": "90",
      "colorrampname": "Orange to Purple",
      "handlerId": "mapView",
      "keyFields": "LATITUDE,LONGITUDE",
      "mapboxtoken": "pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4M29iazA2Z2gycXA4N2pmbDZmangifQ.-g_vE53SD2WrJ6tFX7QHmA",
      "rowCount": "1000",
      "title": "100 Centers of Accidents with Injuries",
      "valueFields": "COUNT"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a Panda DataFrame\n",
    "long=[]\n",
    "lat=[]\n",
    "for center in centers :\n",
    "    long.append(center[0])\n",
    "    lat.append(center[1])\n",
    "\n",
    "summary = model.summary\n",
    "data2 = pd.DataFrame(data={'LONGITUDE': long, 'LATITUDE': lat, \"COUNT\": summary.clusterSizes })\n",
    "display(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means for accidents with fatalities\n",
    "There are only 180 accidents so we'll use a \"k\" of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = spark.createDataFrame(\n",
    "    spark.sql(\"\"\"\n",
    "          select LONGITUDE, LATITUDE from collisions\n",
    "          where LATITUDE is not null\n",
    "          and longitude is not null\n",
    "          and INJURIES_FATAL > 0\n",
    "    \"\"\").rdd.map(lambda r : Row(Vectors.dense([r.LONGITUDE, r.LATITUDE]))), [\"features\"] )\n",
    "\n",
    "kmeans = KMeans(k=10, seed=123)\n",
    "model = kmeans.fit(data1)\n",
    "centers = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "basemap": "outdoors-v9",
      "coloropacity": "89",
      "colorrampname": "Orange to Purple",
      "handlerId": "mapView",
      "keyFields": "LATITUDE,LONGITUDE",
      "mapboxtoken": "pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4M29iazA2Z2gycXA4N2pmbDZmangifQ.-g_vE53SD2WrJ6tFX7QHmA",
      "title": "10 Centers of Accidents with Fatalities",
      "valueFields": "COUNT"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a Panda DataFrame\n",
    "long=[]\n",
    "lat=[]\n",
    "for center in centers :\n",
    "    long.append(center[0])\n",
    "    lat.append(center[1])\n",
    "\n",
    "summary = model.summary\n",
    "data2 = pd.DataFrame(data={'LONGITUDE': long, 'LATITUDE': lat, \"COUNT\": summary.clusterSizes })\n",
    "display(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see %lsmagic for all the commands available\n",
    "%rm $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm $zipfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
